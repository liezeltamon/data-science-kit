---
title: "Machine learning"
format: html
---

## General

#### Terms

- Gradient descent - Training a ML/DL model is an optimization problem that minimizes loss function through gradient descent
  + Variants of gradient descent are: stochastic, batch gradient descent
- Backpropagation - method of calculating gradient of loss function

## Deep learning

#### Loss functions

- Common ones used: cross-entropy and mean-squared error
- Entropy - quantifies how uncertain a model is about its predictions (represented by a probability distribution)
- Cross entropy - commonly used quantifies how well the predicted distribution matches the true distribution

#### Transformers

- Useful particularly for capturing long-range dependencies e.g. relationship between words in a sentence, relationship between regulatory elements in a genomic region

- Terminology
  + Token - piece of input given to transformer (e.g. a word, part of a word)
- Components
  + Encoder
  + Decoder
  + Embedding - convert tokens to numbers
  + Positional encoding - keep track of token order
  + Self-attention - keep track of relationships within the input and output phrases
  + Encoder-Decoder attention - keep track of things between input and output phrases
  + Residual connections
- Questions
  + Application to biology
    + If position between inputs does not matter in application e.g. predicting phenotype from gene/protein expression, is positional encoding omitted? or can it be used to identify relationships important for prediction?
    + For tasks that do not need positional encoding, which is one of the strengths of transformers, why would you prefer transformer? Is it because of the attention mechanism that gives more opportunity to figure out features and relationships between features that are the most important to prediction?
    